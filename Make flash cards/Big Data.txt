Q1. What are the main uses of Kinesis Data Streams?
x They can accept data as soon as it has been produced, without the need for batching
- They can undertake the loading of streamed data directly into data stores
- They can provide long term storage of data
x They can carry out real-time reporting and analysis of streamed data

Q2. For which of the following AWS services can you not create a rule action in AWS IoT?
x Aurora
- Kinesis Streams
x Redshift
- DynamoDB
- CloudWatch
- Kinesis Firehose

Q3. For an unknown reason, data delivery from Kinesis Firehose to your Redshift cluster has failed. Kinesis Firehose retries the data delivery every 5 minutes for a maximum period for of 60 minutes; however, none of the retries deliver the data to Redshift. Kinesis Firehose skips the files and move onto the next batch of files in S3. How can you ensure that the undelivered data is eventually loaded into Redshift?
- You create a Lambda function to automatically load these files into Redshift by reading the manifest after the retries have been completed and the COPY command has been run.
- Check CloudWatch Logs to determine which files in S3 were skipped by Kinesis Firehose, fix the files, and manually load them into Redshift.
- Check the STL_LOAD_ERRORS table in Redshift, find the files that failed to load and manually, and load the data in those files using the COPY command.
x Skipped files are delivered to your S3 bucket as a manifest file in an errors folder. Run the COPY command manually to load the skipped files after you have determined why they failed to load.

Q4. Your company is launching an IoT device that will send data to AWS. All the data generated by the millions of devices your company is going to sell will be stored in DynamoDB for use by the Engineering team. Each customer's data, however, will only be stored in DynamoDB for 30 days. A mobile application will be used to control the IoT device, and easy user sign-up and sign-in to the mobile application are requirements. The engineering team is designing the application to scale to millions of users. Their preference is to not have to worry about building, securing, and scaling authentication for the mobile application. They also want to use their own identity provider. Which option would be the best choice for their mobile application?
- Use a SAML identity provider.
- Since everyone uses Facebook, Amazon, and Google, keep it simple and use all three.
x Use an Amazon Cognito identity pool.
- Use LDAP.

Q5. Regarding SQS, which of the following are true?
x Messages can be retained in queues for up to 14 days.
- A queue can only be created in limited regions, and you should check the SQS website to see which are supported.
x Messages can be sent and read simultaneously.
- Messages can be retained in queues for up to 7 days.
x A queue can be created in any region.

Q6. Which of the following AWS IoT components transforms messages and routes them to different AWS services?
- Device Shadow
x Rules Engine
- Device Gateway
- Rule Actions

Q7. Which service does Kinesis Firehose not load streaming data into?
- S3
- Elasticsearch
x DynamoDB
- Redshift

Q8. Your team has successfully migrated the corporate data warehouse to Redshift. So far, all the data coming into the ETL pipeline for the data warehouse has been from other corporate systems also running on AWS. However, after signing some new business deals with a 3rd party, they will be securely sending files directly to S3. The data in these files needs to be ingested into Redshift. Members of your team are debating the most efficient and best automated way to introduce this change into the ETL pipeline. Which of the following options would you suggest?
- Work with the 3rd party's IT team to install the Data Pipeline Task Runner package, then coordinate a VPN connection from their data center to AWS.
- Procure a new 3rd party tool that integrates with S3 and Redshift that provides powerful scheduling capabilities.
x Use Lambda (AWS Redshift Database Loader).
x Use Data Pipeline.
- Use the SWF service to write a custom workflow to process the incoming files from the 3rd party.
- Run a cron job on a t2.micro instance that will execute Linux shell scripts.


