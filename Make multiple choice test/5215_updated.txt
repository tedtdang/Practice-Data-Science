####Q1. K-means is a supervised learning technique that can be used for classification while K-nearest neighbors is an unsupervised learning technique which returns classes of the samples based solely on similarity between samples
- [ ] True
- [x] False

####Q2. Given the same data set, K-means always converges to the same solution, regardless of the starting point
- [ ] True
- [x] False

####Q3. Check all the ways of initializing K-means
- [x] randomly choose samples as the initial centroids
- [ ] initialize all centroids at the mean of all the samples
- [x] randomly assign all samples to one of K classes
- [ ] initialize all centroids at the origin - e.g. (0,0)

####Q4. Although metrics are available to measure the quality of clustering when we know the true classes, there are no metrics to measure the quality of unsupervised clustering when true classes are not known
- [ ] True
- [x] False

####Q5. Pick the two data formats for use in clustering
- [x] a similarity matrix of size 'samples x samples'
- [ ] a 'features x features' sized correlation matrix
- [x] a 'samples x features' matrix, standard in machine learning but without a chosen predictor

####Q6. Check the scenario where density-based clustering algorithms like DBSCAN are expected to outperform K-means
- [ ] When the clusters are well represented by spheres
- [ ] When we know exactly how many clusters to expect
- [x] When the clusters would share the same center-point, such as two concentric circles

####Q7. Check all the components of a basic reinforcement learning model
- [x] A set of states of the environment
- [x] A set of actions the organism can take
- [x] rules of transitions between states
- [x] rules that determine the immediate reward of certain transitions
- [x] rules that describe what the organism can observe

####Q8. Check all of the following that are associated with"model-free" reinforcement learning as opposed to model-based learning?
- [x] It is more associated with valuation of repetitive events than new, novel environments (e.g. habit learning)
- [ ] It takes advantage of direct knowledge of probabilities between states to optimize learning
- [x] Inferring state and action value functions iteratively based on repeated rewards and punishments
- [x] This technique uses prediction error as the primary means of updating policy decisions

####Q9. An important aspect in formulating a problem as a Markov process is that the future is conditionally independent of the past giving the current state
- [x] True
- [ ] False

####Q10. When you are not sure if your state-action value function is correct, you should always pick the state-action pair of maximum value
- [ ] True
- [x] False

####Q11. Reward prediction error is
- [ ] The total future expected reward minus the total future actual reward (with temporal discounting)
- [ ] The total future actual reward minus the total future expected reward (with temporal discounting)
- [ ] The expected reward value - the received reward value
- [x] The received reward value - the expected reward value

####Q12. In Q learning, you are updating the action value function, but there are two parameters which control the manner in which this updating occurs
- [ ] Regularization strength (lamda)
- [x] Temporal discounting (gamma)
- [ ] Maximum estimation error (epsilon)
- [x] Learning rate (alpha)

