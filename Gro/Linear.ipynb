{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Lightning Data Module (subclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from dask import dataframe as dd\n",
    "import torchvision.models as models\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() // 1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "pl.seed_everything(42)\n",
    "\n",
    "\n",
    "class GroDataset(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = None, batch_size: int = int(2**12), num_workers=NUM_WORKERS):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir or os.getcwd()\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.train = torch.Tensor(dd.read_csv('train.csv'))\n",
    "        self.test = torch.Tensor(dd.read_csv('X_test.csv'))\n",
    "\n",
    "    def setup(self, train_ratio: float = 0.8, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            train_amount = int(len(self.train) * train_ratio)\n",
    "            self.train, self.val = random_split(\n",
    "                self.train, [train_amount, len(self.train) - train_amount])\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Lightning Module subclass (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes, planes, downsample=None):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.silu = nn.SiLU(inplace=True)\n",
    "        self.ln2 = nn.Conv2d(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.ln1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.silu(out)\n",
    "\n",
    "        out = self.ln2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.silu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "class ResNetRegressor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__()\n",
    "        self.lr = 1e-3\n",
    "        self.loss = mean_absolute_percentage_error()\n",
    "        self.sl = nn.SiLU()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = \n",
    "\n",
    "    def _make_layer(self, block,inplanes,planes, blocks):\n",
    "        downsample = None\n",
    "        if inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Linear(inplanes, planes),\n",
    "                nn.BatchNorm1d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, downsample))\n",
    "        inplanes = planes\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, X):\n",
    "        return self.resnet_model(X)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self.forward(x)\n",
    "        if self.num_classes == 2:\n",
    "            y = F.one_hot(y, num_classes=2).float()\n",
    "        loss = self.loss(preds, y)\n",
    "        acc = self.acc(preds, y)\n",
    "        # Logging the loss\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self.forward(x)\n",
    "        if self.num_classes == 2:\n",
    "            y = F.one_hot(y, num_classes=2).float()\n",
    "        loss = self.loss(preds, y)\n",
    "        acc = self.acc(preds, y)\n",
    "        # Logging the loss\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        if self.num_classes == 2:\n",
    "            y = F.one_hot(y, num_classes=2).float()\n",
    "        loss = self.loss(preds, y)\n",
    "        acc = self.acc(preds, y)\n",
    "        # perform logging\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, on_step=True, logger=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abf09f5f6aa7afbe2342614c81cd1c1b856e5671cbcd1665006873764f7b3ab6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
