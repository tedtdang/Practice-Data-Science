{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessory-night",
   "metadata": {},
   "source": [
    "### This is from a Kaggle competition: https://www.kaggle.com/c/nlp-getting-started/data?select=train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "careful-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-taiwan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-savings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_dbdbb_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >missing</th>        <th class=\"col_heading level0 col1\" >total</th>        <th class=\"col_heading level0 col2\" >percent</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_dbdbb_level0_row0\" class=\"row_heading level0 row0\" >location</th>\n",
       "                        <td id=\"T_dbdbb_row0_col0\" class=\"data row0 col0\" >2,533</td>\n",
       "                        <td id=\"T_dbdbb_row0_col1\" class=\"data row0 col1\" >7,613</td>\n",
       "                        <td id=\"T_dbdbb_row0_col2\" class=\"data row0 col2\" >33.27%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbdbb_level0_row1\" class=\"row_heading level0 row1\" >keyword</th>\n",
       "                        <td id=\"T_dbdbb_row1_col0\" class=\"data row1 col0\" >61</td>\n",
       "                        <td id=\"T_dbdbb_row1_col1\" class=\"data row1 col1\" >7,613</td>\n",
       "                        <td id=\"T_dbdbb_row1_col2\" class=\"data row1 col2\" >0.80%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbdbb_level0_row2\" class=\"row_heading level0 row2\" >id</th>\n",
       "                        <td id=\"T_dbdbb_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "                        <td id=\"T_dbdbb_row2_col1\" class=\"data row2 col1\" >7,613</td>\n",
       "                        <td id=\"T_dbdbb_row2_col2\" class=\"data row2 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbdbb_level0_row3\" class=\"row_heading level0 row3\" >text</th>\n",
       "                        <td id=\"T_dbdbb_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "                        <td id=\"T_dbdbb_row3_col1\" class=\"data row3 col1\" >7,613</td>\n",
       "                        <td id=\"T_dbdbb_row3_col2\" class=\"data row3 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_dbdbb_level0_row4\" class=\"row_heading level0 row4\" >target</th>\n",
       "                        <td id=\"T_dbdbb_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "                        <td id=\"T_dbdbb_row4_col1\" class=\"data row4 col1\" >7,613</td>\n",
       "                        <td id=\"T_dbdbb_row4_col2\" class=\"data row4 col2\" >0.00%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x203e9dd7b08>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sidetable\n",
    "df_train.stb.missing(style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clinical-fitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(df_train.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polish-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "def get_pad_sq(df):\n",
    "    text = np.array(df.text)\n",
    "    tokenizer = Tokenizer(num_words=len(df.text.unique()))\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    # Find the max length of rows in sequences\n",
    "    max_length = 0\n",
    "    for sentence in sequences:\n",
    "        max_length = max(max_length, len(sentence))\n",
    "    df = sequence.pad_sequences(sequences, maxlen=max_length)\n",
    "    return (df, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addressed-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = get_pad_sq(df_train)\n",
    "test, max_review_length = get_pad_sq(df_test)\n",
    "train_target = np.array(df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, train_target, stratify=train_target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-custom",
   "metadata": {},
   "source": [
    "Credit : https://medium.com/@mrunal68/text-sentiments-classification-with-cnn-and-lstm-f92652bc29fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "individual-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 30, 128)           1280000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 30, 32)            12320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 15, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,345,621\n",
      "Trainable params: 1,345,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4000\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 30) for input KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 32).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 30) for input KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 32).\n",
      "191/191 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.6260WARNING:tensorflow:Model was constructed with shape (None, 30) for input KerasTensor(type_spec=TensorSpec(shape=(None, 30), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 32).\n",
      "191/191 [==============================] - 11s 38ms/step - loss: 0.6223 - accuracy: 0.6264 - val_loss: 0.4426 - val_accuracy: 0.8030\n",
      "Epoch 2/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.3349 - accuracy: 0.8703 - val_loss: 0.4638 - val_accuracy: 0.8050\n",
      "Epoch 3/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.2112 - accuracy: 0.9230 - val_loss: 0.5436 - val_accuracy: 0.7958\n",
      "Epoch 4/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.1378 - accuracy: 0.9515 - val_loss: 0.6103 - val_accuracy: 0.7748\n",
      "Epoch 5/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0984 - accuracy: 0.9653 - val_loss: 0.8122 - val_accuracy: 0.7787\n",
      "Epoch 6/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0744 - accuracy: 0.9739 - val_loss: 0.8194 - val_accuracy: 0.7610\n",
      "Epoch 7/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0629 - accuracy: 0.9768 - val_loss: 1.0370 - val_accuracy: 0.7643\n",
      "Epoch 8/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 1.0584 - val_accuracy: 0.7748\n",
      "Epoch 9/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0508 - accuracy: 0.9799 - val_loss: 1.2339 - val_accuracy: 0.7649\n",
      "Epoch 10/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0416 - accuracy: 0.9840 - val_loss: 1.4951 - val_accuracy: 0.7643\n",
      "Epoch 11/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0363 - accuracy: 0.9825 - val_loss: 1.4157 - val_accuracy: 0.7630\n",
      "Epoch 12/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0250 - accuracy: 0.9876 - val_loss: 1.4375 - val_accuracy: 0.7564\n",
      "Epoch 13/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0291 - accuracy: 0.9857 - val_loss: 1.5268 - val_accuracy: 0.7505\n",
      "Epoch 14/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0339 - accuracy: 0.9826 - val_loss: 1.7390 - val_accuracy: 0.7492\n",
      "Epoch 15/4000\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.0301 - accuracy: 0.9864 - val_loss: 1.5557 - val_accuracy: 0.7617\n",
      "Epoch 16/4000\n",
      "191/191 [==============================] - 7s 36ms/step - loss: 0.0282 - accuracy: 0.9859 - val_loss: 1.6562 - val_accuracy: 0.7538\n",
      "Epoch 17/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0304 - accuracy: 0.9865 - val_loss: 1.5204 - val_accuracy: 0.7557\n",
      "Epoch 18/4000\n",
      "191/191 [==============================] - 7s 36ms/step - loss: 0.0287 - accuracy: 0.9866 - val_loss: 1.5741 - val_accuracy: 0.7663\n",
      "Epoch 19/4000\n",
      "191/191 [==============================] - 7s 37ms/step - loss: 0.0271 - accuracy: 0.9844 - val_loss: 1.6521 - val_accuracy: 0.7649\n",
      "Epoch 20/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0259 - accuracy: 0.9860 - val_loss: 1.6037 - val_accuracy: 0.7597\n",
      "Epoch 21/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0303 - accuracy: 0.9854 - val_loss: 1.5852 - val_accuracy: 0.7610\n",
      "Epoch 22/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0285 - accuracy: 0.9868 - val_loss: 1.4600 - val_accuracy: 0.7663\n",
      "Epoch 23/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0307 - accuracy: 0.9834 - val_loss: 1.7491 - val_accuracy: 0.7695\n",
      "Epoch 24/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0238 - accuracy: 0.9876 - val_loss: 1.6046 - val_accuracy: 0.7722\n",
      "Epoch 25/4000\n",
      "191/191 [==============================] - 7s 39ms/step - loss: 0.0252 - accuracy: 0.9871 - val_loss: 1.7577 - val_accuracy: 0.7702\n",
      "Epoch 26/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0269 - accuracy: 0.9870 - val_loss: 1.6809 - val_accuracy: 0.7663\n",
      "Epoch 27/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0224 - accuracy: 0.9895 - val_loss: 1.6750 - val_accuracy: 0.7643\n",
      "Epoch 28/4000\n",
      "191/191 [==============================] - 7s 35ms/step - loss: 0.0223 - accuracy: 0.9887 - val_loss: 1.6635 - val_accuracy: 0.7643\n",
      "Epoch 29/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0235 - accuracy: 0.9872 - val_loss: 1.8732 - val_accuracy: 0.7643\n",
      "Epoch 30/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0261 - accuracy: 0.9856 - val_loss: 1.7975 - val_accuracy: 0.7623\n",
      "Epoch 31/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0254 - accuracy: 0.9870 - val_loss: 1.7216 - val_accuracy: 0.7617\n",
      "Epoch 32/4000\n",
      "191/191 [==============================] - 7s 34ms/step - loss: 0.0226 - accuracy: 0.9882 - val_loss: 1.8219 - val_accuracy: 0.7623\n",
      "Epoch 33/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0228 - accuracy: 0.9865 - val_loss: 1.9765 - val_accuracy: 0.7643\n",
      "Epoch 34/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0217 - accuracy: 0.9876 - val_loss: 2.0056 - val_accuracy: 0.7636\n",
      "Epoch 35/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0234 - accuracy: 0.9883 - val_loss: 1.5557 - val_accuracy: 0.7610\n",
      "Epoch 36/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0277 - accuracy: 0.9853 - val_loss: 1.4440 - val_accuracy: 0.7564\n",
      "Epoch 37/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0230 - accuracy: 0.9879 - val_loss: 1.5728 - val_accuracy: 0.7617\n",
      "Epoch 38/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0230 - accuracy: 0.9881 - val_loss: 1.7190 - val_accuracy: 0.7610\n",
      "Epoch 39/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0225 - accuracy: 0.9858 - val_loss: 1.6764 - val_accuracy: 0.7630\n",
      "Epoch 40/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0233 - accuracy: 0.9886 - val_loss: 1.6786 - val_accuracy: 0.7584\n",
      "Epoch 41/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0265 - accuracy: 0.9882 - val_loss: 1.4569 - val_accuracy: 0.7656\n",
      "Epoch 42/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0250 - accuracy: 0.9863 - val_loss: 1.6011 - val_accuracy: 0.7597\n",
      "Epoch 43/4000\n",
      "191/191 [==============================] - 6s 32ms/step - loss: 0.0237 - accuracy: 0.9867 - val_loss: 1.7667 - val_accuracy: 0.7663\n",
      "Epoch 44/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0228 - accuracy: 0.9867 - val_loss: 1.7953 - val_accuracy: 0.7590\n",
      "Epoch 45/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0228 - accuracy: 0.9872 - val_loss: 1.7913 - val_accuracy: 0.7557\n",
      "Epoch 46/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0229 - accuracy: 0.9857 - val_loss: 1.8387 - val_accuracy: 0.7577\n",
      "Epoch 47/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0250 - accuracy: 0.9858 - val_loss: 1.8940 - val_accuracy: 0.7557\n",
      "Epoch 48/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0213 - accuracy: 0.9885 - val_loss: 1.9286 - val_accuracy: 0.7590\n",
      "Epoch 49/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0210 - accuracy: 0.9879 - val_loss: 1.5681 - val_accuracy: 0.7590\n",
      "Epoch 50/4000\n",
      "191/191 [==============================] - 6s 34ms/step - loss: 0.0243 - accuracy: 0.9870 - val_loss: 1.6904 - val_accuracy: 0.7472\n",
      "Epoch 51/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0214 - accuracy: 0.9873 - val_loss: 1.5895 - val_accuracy: 0.7597\n",
      "Epoch 52/4000\n",
      "191/191 [==============================] - 6s 33ms/step - loss: 0.0241 - accuracy: 0.9878 - val_loss: 1.6295 - val_accuracy: 0.7466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x203fac24b08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# create the model\n",
    "embedding_vector_length = 128\n",
    "top_words = 10000\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy',verbose=0, save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_accuracy', verbose=0, patience=50)\n",
    "callbacks_list = [checkpoint, es]\n",
    "model.fit(X_train, y_train, epochs=4000, batch_size=32,verbose = 1,callbacks = callbacks_list,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-measurement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gputest] *",
   "language": "python",
   "name": "conda-env-gputest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
